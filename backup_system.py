"""
H·ªá th·ªëng Sao l∆∞u T·ª± ƒë·ªông - Airbnb WebApp
Backup PostgreSQL database v·ªõi retention policy v√† monitoring
"""

import os
import subprocess
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import schedule
import time
from pathlib import Path

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('logs/backup.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DatabaseBackup:
    """Qu·∫£n l√Ω backup database PostgreSQL t·ª± ƒë·ªông"""
    
    def __init__(self, config_file: str = "backup_config.json"):
        """Kh·ªüi t·∫°o backup system"""
        self.config = self.load_config(config_file)
        self.backup_dir = Path(self.config.get("backup_directory", "backups/database"))
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("üîÑ Kh·ªüi t·∫°o Database Backup System")
        logger.info(f"üìÅ Backup directory: {self.backup_dir}")
    
    def load_config(self, config_file: str) -> Dict:
        """Load c·∫•u h√¨nh backup"""
        default_config = {
            "database_url": os.getenv("DATABASE_URL", ""),
            "backup_directory": "backups/database",
            "retention_days": 30,
            "backup_schedule": {
                "daily": "02:00",  # 2 AM h√†ng ng√†y
                "weekly": "sunday:03:00",  # Ch·ªß nh·∫≠t 3 AM
                "monthly": "1:04:00"  # Ng√†y 1 h√†ng th√°ng 4 AM
            },
            "compression": True,
            "encryption": False,
            "notification": {
                "enabled": False,
                "webhook_url": "",
                "email": ""
            }
        }
        
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            else:
                # T·∫°o config file m·∫∑c ƒë·ªãnh
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2, ensure_ascii=False)
                logger.info(f"üìÑ ƒê√£ t·∫°o config file m·∫∑c ƒë·ªãnh: {config_file}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói load config, s·ª≠ d·ª•ng default: {e}")
        
        return default_config
    
    def create_backup(self, backup_type: str = "manual") -> Optional[str]:
        """T·∫°o backup database"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"airbnb_db_backup_{backup_type}_{timestamp}.sql"
            
            if self.config.get("compression", True):
                backup_filename += ".gz"
            
            backup_path = self.backup_dir / backup_filename
            
            logger.info(f"üîÑ B·∫Øt ƒë·∫ßu backup {backup_type}...")
            logger.info(f"üìÅ File: {backup_path}")
            
            # Parse database URL
            db_url = self.config["database_url"]
            if not db_url.startswith("postgresql"):
                logger.error("‚ùå Ch·ªâ h·ªó tr·ª£ PostgreSQL backup")
                return None
            
            # Extract connection info
            # postgresql://user:password@host:port/database
            parts = db_url.replace("postgresql://", "").split("/")
            database = parts[1] if len(parts) > 1 else "airbnb_db"
            user_host = parts[0].split("@")
            host_port = user_host[1] if len(user_host) > 1 else "localhost:5432"
            user_pass = user_host[0].split(":")
            
            host = host_port.split(":")[0]
            port = host_port.split(":")[1] if ":" in host_port else "5432"
            username = user_pass[0]
            password = user_pass[1] if len(user_pass) > 1 else ""
            
            # T·∫°o command pg_dump
            cmd = [
                "pg_dump",
                f"--host={host}",
                f"--port={port}",
                f"--username={username}",
                f"--dbname={database}",
                "--verbose",
                "--clean",
                "--no-owner",
                "--no-privileges",
                "--format=custom"
            ]
            
            # Set password environment
            env = os.environ.copy()
            if password:
                env["PGPASSWORD"] = password
            
            # Ch·∫°y backup
            if self.config.get("compression", True):
                with open(backup_path, 'wb') as f:
                    import gzip
                    with gzip.open(backup_path, 'wb') as gz_f:
                        process = subprocess.run(
                            cmd, stdout=gz_f, stderr=subprocess.PIPE,
                            env=env, check=True
                        )
            else:
                with open(backup_path, 'wb') as f:
                    process = subprocess.run(
                        cmd, stdout=f, stderr=subprocess.PIPE,
                        env=env, check=True
                    )
            
            # Ki·ªÉm tra k·∫øt qu·∫£
            if backup_path.exists() and backup_path.stat().st_size > 0:
                file_size = backup_path.stat().st_size / (1024 * 1024)  # MB
                logger.info(f"‚úÖ Backup th√†nh c√¥ng!")
                logger.info(f"üìä K√≠ch th∆∞·ªõc: {file_size:.2f} MB")
                
                # Ghi metadata
                metadata = {
                    "backup_type": backup_type,
                    "timestamp": timestamp,
                    "database": database,
                    "file_size_mb": round(file_size, 2),
                    "compressed": self.config.get("compression", True),
                    "created_at": datetime.now().isoformat()
                }
                
                metadata_file = backup_path.with_suffix('.json')
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                
                return str(backup_path)
            else:
                logger.error("‚ùå Backup file tr·ªëng ho·∫∑c kh√¥ng t·∫°o ƒë∆∞·ª£c")
                return None
                
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå L·ªói pg_dump: {e.stderr.decode()}")
            return None
        except Exception as e:
            logger.error(f"‚ùå L·ªói t·∫°o backup: {e}")
            return None
    
    def cleanup_old_backups(self):
        """X√≥a backup c≈© theo retention policy"""
        try:
            retention_days = self.config.get("retention_days", 30)
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            logger.info(f"üßπ D·ªçn d·∫πp backup c≈© h∆°n {retention_days} ng√†y...")
            
            deleted_count = 0
            for backup_file in self.backup_dir.glob("*.sql*"):
                if backup_file.stat().st_mtime < cutoff_date.timestamp():
                    try:
                        # X√≥a c·∫£ file backup v√† metadata
                        backup_file.unlink()
                        metadata_file = backup_file.with_suffix('.json')
                        if metadata_file.exists():
                            metadata_file.unlink()
                        
                        deleted_count += 1
                        logger.info(f"üóëÔ∏è ƒê√£ x√≥a: {backup_file.name}")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a {backup_file.name}: {e}")
            
            if deleted_count > 0:
                logger.info(f"‚úÖ ƒê√£ x√≥a {deleted_count} backup c≈©")
            else:
                logger.info("‚úÖ Kh√¥ng c√≥ backup c≈© c·∫ßn x√≥a")
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói cleanup backup: {e}")
    
    def verify_backup(self, backup_path: str) -> bool:
        """Verify backup integrity"""
        try:
            logger.info(f"üîç Verify backup: {backup_path}")
            
            if not os.path.exists(backup_path):
                logger.error("‚ùå Backup file kh√¥ng t·ªìn t·∫°i")
                return False
            
            # Ki·ªÉm tra size
            file_size = os.path.getsize(backup_path)
            if file_size < 1024:  # < 1KB
                logger.error("‚ùå Backup file qu√° nh·ªè")
                return False
            
            # Test decompression n·∫øu compressed
            if backup_path.endswith('.gz'):
                import gzip
                try:
                    with gzip.open(backup_path, 'rb') as f:
                        f.read(1024)  # ƒê·ªçc 1KB ƒë·∫ßu
                except Exception as e:
                    logger.error(f"‚ùå L·ªói decompression: {e}")
                    return False
            
            logger.info("‚úÖ Backup verification passed")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói verify backup: {e}")
            return False
    
    def list_backups(self) -> List[Dict]:
        """Li·ªát k√™ t·∫•t c·∫£ backup"""
        backups = []
        
        for backup_file in sorted(self.backup_dir.glob("*.sql*")):
            metadata_file = backup_file.with_suffix('.json')
            
            backup_info = {
                "filename": backup_file.name,
                "path": str(backup_file),
                "size_mb": round(backup_file.stat().st_size / (1024 * 1024), 2),
                "created": datetime.fromtimestamp(backup_file.stat().st_mtime),
                "metadata": None
            }
            
            # Load metadata n·∫øu c√≥
            if metadata_file.exists():
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        backup_info["metadata"] = json.load(f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c metadata cho {backup_file.name}: {e}")
            
            backups.append(backup_info)
        
        return backups
    
    def restore_backup(self, backup_path: str, target_db: str = None) -> bool:
        """Restore backup (c·∫©n th·∫≠n s·ª≠ d·ª•ng!)"""
        try:
            if not target_db:
                target_db = self.config["database_url"]
            
            logger.warning("‚ö†Ô∏è C·∫¢NH B√ÅO: ƒêang restore database - d·ªØ li·ªáu hi·ªán t·∫°i s·∫Ω b·ªã ghi ƒë√®!")
            
            # Verify backup tr∆∞·ªõc
            if not self.verify_backup(backup_path):
                logger.error("‚ùå Backup verification th·∫•t b·∫°i")
                return False
            
            # T·∫°o pg_restore command
            # (Implementation t√πy thu·ªôc v√†o format backup)
            logger.info("üîÑ Restore backup s·∫Ω ƒë∆∞·ª£c implement chi ti·∫øt...")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói restore: {e}")
            return False
    
    def schedule_backups(self):
        """Setup scheduled backups"""
        try:
            schedule_config = self.config.get("backup_schedule", {})
            
            # Daily backup
            if "daily" in schedule_config:
                time_str = schedule_config["daily"]
                schedule.every().day.at(time_str).do(
                    lambda: self.create_backup("daily")
                )
                logger.info(f"üìÖ Scheduled daily backup at {time_str}")
            
            # Weekly backup
            if "weekly" in schedule_config:
                day_time = schedule_config["weekly"]
                if ":" in day_time:
                    day, time_str = day_time.split(":", 1)
                    getattr(schedule.every(), day.lower()).at(time_str).do(
                        lambda: self.create_backup("weekly")
                    )
                    logger.info(f"üìÖ Scheduled weekly backup on {day} at {time_str}")
            
            # Cleanup schedule
            schedule.every().day.at("05:00").do(self.cleanup_old_backups)
            logger.info("üìÖ Scheduled daily cleanup at 05:00")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói setup schedule: {e}")
    
    def run_scheduler(self):
        """Ch·∫°y scheduled backup daemon"""
        logger.info("üîÑ B·∫Øt ƒë·∫ßu backup scheduler...")
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # Check m·ªói ph√∫t
            except KeyboardInterrupt:
                logger.info("‚èπÔ∏è D·ª´ng backup scheduler")
                break
            except Exception as e:
                logger.error(f"‚ùå L·ªói scheduler: {e}")
                time.sleep(300)  # Sleep 5 ph√∫t n·∫øu l·ªói

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Database Backup System")
    parser.add_argument("--action", choices=["backup", "list", "cleanup", "schedule"], 
                       default="backup", help="H√†nh ƒë·ªông c·∫ßn th·ª±c hi·ªán")
    parser.add_argument("--type", choices=["manual", "daily", "weekly", "monthly"], 
                       default="manual", help="Lo·∫°i backup")
    parser.add_argument("--config", default="backup_config.json", help="Config file")
    
    args = parser.parse_args()
    
    # T·∫°o backup system
    backup_system = DatabaseBackup(args.config)
    
    if args.action == "backup":
        result = backup_system.create_backup(args.type)
        if result:
            logger.info(f"‚úÖ Backup ho√†n t·∫•t: {result}")
        else:
            logger.error("‚ùå Backup th·∫•t b·∫°i")
    
    elif args.action == "list":
        backups = backup_system.list_backups()
        print("\nüìã DANH S√ÅCH BACKUP:")
        print("-" * 60)
        for backup in backups:
            print(f"üìÅ {backup['filename']}")
            print(f"   K√≠ch th∆∞·ªõc: {backup['size_mb']} MB")
            print(f"   T·∫°o l√∫c: {backup['created']}")
            if backup['metadata']:
                print(f"   Lo·∫°i: {backup['metadata'].get('backup_type', 'Unknown')}")
            print()
    
    elif args.action == "cleanup":
        backup_system.cleanup_old_backups()
    
    elif args.action == "schedule":
        backup_system.schedule_backups()
        backup_system.run_scheduler()

if __name__ == "__main__":
    main()