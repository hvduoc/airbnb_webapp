#!/usr/bin/env python3
"""
AIRBNB WEBAPP - Database Index Monitoring System
H·ªá th·ªëng gi√°m s√°t v√† t·ªëi ∆∞u h√≥a indexes
Author: AI Assistant
Created: 2024-12-28
"""

import json
import logging
from datetime import datetime
from typing import Any, Dict, List

from sqlmodel import Session, text

from db import get_session_context

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseIndexMonitor:
    """Monitor v√† ph√¢n t√≠ch hi·ªáu qu·∫£ c·ªßa database indexes"""
    
    def __init__(self):
        self.index_stats = {}
        self.recommendations = []
    
    def analyze_index_usage(self, session: Session) -> Dict[str, Any]:
        """Ph√¢n t√≠ch vi·ªác s·ª≠ d·ª•ng indexes"""
        
        # Query ƒë·ªÉ l·∫•y th·ªëng k√™ index usage (PostgreSQL specific)
        index_usage_query = text("""
            SELECT 
                schemaname,
                tablename,
                indexname,
                idx_scan as index_scans,
                idx_tup_read as tuples_read,
                idx_tup_fetch as tuples_fetched,
                CASE 
                    WHEN idx_scan = 0 THEN 'UNUSED'
                    WHEN idx_scan < 10 THEN 'LOW_USAGE'
                    WHEN idx_scan < 100 THEN 'MODERATE_USAGE'
                    ELSE 'HIGH_USAGE'
                END as usage_level
            FROM pg_stat_user_indexes
            WHERE schemaname = 'public'
            ORDER BY idx_scan DESC
        """)
        
        try:
            results = session.exec(index_usage_query).all()
            
            index_stats = []
            for row in results:
                index_stats.append({
                    'schema': row[0],
                    'table': row[1], 
                    'index': row[2],
                    'scans': row[3],
                    'tuples_read': row[4],
                    'tuples_fetched': row[5],
                    'usage_level': row[6]
                })
            
            self.index_stats['usage'] = index_stats
            return {'index_usage': index_stats}
            
        except Exception as e:
            logger.warning(f"PostgreSQL index stats kh√¥ng kh·∫£ d·ª•ng: {e}")
            return {'index_usage': 'Not available - requires PostgreSQL'}
    
    def analyze_table_sizes(self, session: Session) -> Dict[str, Any]:
        """Ph√¢n t√≠ch k√≠ch th∆∞·ªõc tables v√† indexes"""
        
        table_size_query = text("""
            SELECT 
                tablename,
                pg_size_pretty(pg_total_relation_size(tablename::regclass)) as total_size,
                pg_size_pretty(pg_relation_size(tablename::regclass)) as table_size,
                pg_size_pretty(pg_indexes_size(tablename::regclass)) as indexes_size,
                pg_stat_get_tuples_returned(c.oid) as tuples_returned,
                pg_stat_get_tuples_fetched(c.oid) as tuples_fetched
            FROM pg_tables t
            JOIN pg_class c ON c.relname = t.tablename
            WHERE schemaname = 'public'
            ORDER BY pg_total_relation_size(tablename::regclass) DESC
        """)
        
        try:
            results = session.exec(table_size_query).all()
            
            table_stats = []
            for row in results:
                table_stats.append({
                    'table': row[0],
                    'total_size': row[1],
                    'table_size': row[2], 
                    'indexes_size': row[3],
                    'tuples_returned': row[4],
                    'tuples_fetched': row[5]
                })
                
            self.index_stats['table_sizes'] = table_stats
            return {'table_sizes': table_stats}
            
        except Exception as e:
            logger.warning(f"PostgreSQL table stats kh√¥ng kh·∫£ d·ª•ng: {e}")
            return {'table_sizes': 'Not available - requires PostgreSQL'}
    
    def check_missing_indexes(self, session: Session) -> List[str]:
        """Ki·ªÉm tra c√°c indexes c√≥ th·ªÉ b·ªã thi·∫øu"""
        
        missing_indexes_query = text("""
            SELECT 
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation
            FROM pg_stats 
            WHERE schemaname = 'public'
            AND n_distinct > 10  -- Columns with good selectivity
            AND correlation < 0.1  -- Columns with low correlation
            ORDER BY n_distinct DESC
        """)
        
        recommendations = []
        
        try:
            results = session.exec(missing_indexes_query).all()
            
            for row in results:
                table = row[1]
                column = row[2]
                n_distinct = row[3]
                
                # Check if index already exists
                index_check = text(f"""
                    SELECT indexname 
                    FROM pg_indexes 
                    WHERE tablename = '{table}' 
                    AND indexdef LIKE '%{column}%'
                """)
                
                existing = session.exec(index_check).first()
                
                if not existing and n_distinct > 50:
                    recommendations.append(
                        f"Xem x√©t t·∫°o index cho {table}.{column} (n_distinct: {n_distinct})"
                    )
                    
        except Exception as e:
            logger.warning(f"Missing index analysis kh√¥ng kh·∫£ d·ª•ng: {e}")
            recommendations.append("Missing index analysis - requires PostgreSQL")
        
        self.recommendations.extend(recommendations)
        return recommendations
    
    def analyze_slow_queries(self, session: Session) -> List[str]:
        """Ph√¢n t√≠ch slow queries c√≥ th·ªÉ c·∫ßn indexes"""
        
        # Ki·ªÉm tra pg_stat_statements extension
        slow_query_analysis = text("""
            SELECT 
                query,
                calls,
                total_time,
                mean_time,
                rows
            FROM pg_stat_statements 
            WHERE mean_time > 100  -- Queries > 100ms
            ORDER BY mean_time DESC
            LIMIT 10
        """)
        
        recommendations = []
        
        try:
            results = session.exec(slow_query_analysis).all()
            
            for row in results:
                query = row[0][:100] + "..." if len(row[0]) > 100 else row[0]
                mean_time = row[3]
                
                recommendations.append(
                    f"Slow query ({mean_time:.2f}ms): {query}"
                )
                
        except Exception as e:
            logger.warning(f"Slow query analysis kh√¥ng kh·∫£ d·ª•ng: {e}")
            recommendations.append("Slow query analysis - requires pg_stat_statements extension")
        
        self.recommendations.extend(recommendations)
        return recommendations
    
    def generate_index_recommendations(self) -> List[str]:
        """T·∫°o c√°c khuy·∫øn ngh·ªã t·ªëi ∆∞u index"""
        
        general_recommendations = [
            "üìä KHUY·∫æN NGH·ªä T·ªîNG QU√ÅT:",
            "",
            "‚úÖ Indexes ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u cho:",
            "   ‚Ä¢ User model: role, is_active, last_login composite indexes",
            "   ‚Ä¢ Property model: building_id, room_type, price_per_night indexes", 
            "   ‚Ä¢ Booking model: date ranges, property availability, revenue queries",
            "   ‚Ä¢ Expense model: category, vendor, allocation method indexes",
            "",
            "üîß KHUY·∫æN NGH·ªä B·∫¢O TR√å:",
            "   ‚Ä¢ Ch·∫°y ANALYZE ƒë·ªãnh k·ª≥ ƒë·ªÉ c·∫≠p nh·∫≠t statistics",
            "   ‚Ä¢ Monitor index usage v·ªõi pg_stat_user_indexes",
            "   ‚Ä¢ Xem x√©t REINDEX cho tables l·ªõn ƒë·ªãnh k·ª≥",
            "   ‚Ä¢ Ki·ªÉm tra disk space cho index growth",
            "",
            "‚ö° KHUY·∫æN NGH·ªä HI·ªÜU SU·∫§T:",
            "   ‚Ä¢ S·ª≠ d·ª•ng composite indexes cho multi-column WHERE clauses",
            "   ‚Ä¢ Index foreign keys ƒë·ªÉ t·ªëi ∆∞u JOINs",
            "   ‚Ä¢ Partial indexes cho filtered queries th∆∞·ªùng xuy√™n",
            "   ‚Ä¢ VACUUM ANALYZE sau bulk operations",
        ]
        
        return general_recommendations + self.recommendations
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß v·ªÅ indexes"""
        logger.info("üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch Database Indexes...")
        
        analysis_results = {}
        
        with get_session_context() as session:
            # Index usage analysis
            analysis_results.update(self.analyze_index_usage(session))
            
            # Table size analysis  
            analysis_results.update(self.analyze_table_sizes(session))
            
            # Missing index recommendations
            missing_indexes = self.check_missing_indexes(session)
            analysis_results['missing_indexes'] = missing_indexes
            
            # Slow query analysis
            slow_queries = self.analyze_slow_queries(session)
            analysis_results['slow_queries'] = slow_queries
            
            # General recommendations
            recommendations = self.generate_index_recommendations()
            analysis_results['recommendations'] = recommendations
        
        return analysis_results
    
    def generate_monitoring_report(self, analysis_results: Dict[str, Any]) -> str:
        """T·∫°o b√°o c√°o monitoring chi ti·∫øt"""
        
        report = "\n" + "="*70 + "\n"
        report += "üìà DATABASE INDEX MONITORING REPORT\n"
        report += "="*70 + "\n"
        report += f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # Index Usage Section
        if 'index_usage' in analysis_results:
            report += "üîç INDEX USAGE ANALYSIS:\n"
            report += "-" * 50 + "\n"
            
            usage_data = analysis_results['index_usage']
            if isinstance(usage_data, str):
                report += f"   {usage_data}\n\n"
            else:
                for idx in usage_data[:10]:  # Top 10 indexes
                    report += f"   üìä {idx['table']}.{idx['index']}\n"
                    report += f"      Scans: {idx['scans']}, Usage: {idx['usage_level']}\n"
                report += "\n"
        
        # Table Sizes Section  
        if 'table_sizes' in analysis_results:
            report += "üíæ TABLE SIZE ANALYSIS:\n"
            report += "-" * 50 + "\n"
            
            size_data = analysis_results['table_sizes']
            if isinstance(size_data, str):
                report += f"   {size_data}\n\n"
            else:
                for table in size_data[:5]:  # Top 5 largest tables
                    report += f"   üì¶ {table['table']}\n"
                    report += f"      Total: {table['total_size']}, "
                    report += f"Indexes: {table['indexes_size']}\n"
                report += "\n"
        
        # Recommendations Section
        if 'recommendations' in analysis_results:
            report += "üí° KHUY·∫æN NGH·ªä:\n"
            report += "-" * 50 + "\n"
            for rec in analysis_results['recommendations']:
                report += f"   {rec}\n"
            report += "\n"
        
        report += "="*70 + "\n"
        
        return report

def main():
    """Main function ƒë·ªÉ ch·∫°y index monitoring"""
    monitor = DatabaseIndexMonitor()
    
    try:
        # Ch·∫°y full analysis
        results = monitor.run_full_analysis()
        
        # T·∫°o v√† hi·ªÉn th·ªã report
        report = monitor.generate_monitoring_report(results)
        print(report)
        
        # L∆∞u detailed results
        with open('index_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str, ensure_ascii=False)
        
        # L∆∞u report
        with open('index_monitoring_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("‚úÖ Index monitoring ho√†n th√†nh!")
        logger.info("üìÑ B√°o c√°o chi ti·∫øt: index_monitoring_report.txt")
        logger.info("üìä D·ªØ li·ªáu JSON: index_analysis.json")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh monitoring: {e}")
        raise

if __name__ == "__main__":
    main()